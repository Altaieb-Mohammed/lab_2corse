{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPiTuqv8pETcbx2ke0t2C47",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Altaieb-Mohammed/lab_2corse/blob/master/lab6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------------------------------------------- Лабораторная работа №6: кластеризация и анализ качества\n",
        "\n",
        "# Импортируем библиотеки для работы с данными, визуализацией и кластеризацией\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN, OPTICS\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from scipy.cluster.hierarchy import dendrogram, linkage\n",
        "from scipy.spatial.distance import cdist\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Загружаем данные с GitHub и выбираем числовые признаки для анализа\n",
        "url = \"https://raw.githubusercontent.com/Altaieb-Mohammed/lab_2corse/master/inheritance_combined.csv\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "feature_columns = [\n",
        "    'debts', 'bequests', 'wife', 'husband',\n",
        "    'sons', 'daughters', 'father', 'mother',\n",
        "    'brothers', 'sisters'\n",
        "]\n",
        "\n",
        "# Выбираем признаки и заполняем пропуски медианой\n",
        "X = df[feature_columns].copy()\n",
        "X.fillna(X.median(), inplace=True)\n"
      ],
      "metadata": {
        "id": "UCEHtQ9wOEdF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Масштабируем признаки для корректной работы кластерных алгоритмов\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Метод локтя для выбора оптимального числа кластеров в KMeans\n",
        "inertias = []\n",
        "k_range = range(1, 11)\n",
        "for k in k_range:\n",
        "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
        "    kmeans.fit(X_scaled)\n",
        "    inertias.append(kmeans.inertia_)\n",
        "\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(k_range, inertias, marker='o')\n",
        "plt.title(\"Метод локтя для определения оптимального k\")\n",
        "plt.xlabel(\"Количество кластеров (k)\")\n",
        "plt.ylabel(\"Инерция (сумма квадратов расстояний)\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "a3u8cFZrOICM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Запускаем KMeans с выбранным числом кластеров (3)\n",
        "kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)\n",
        "clusters = kmeans.fit_predict(X_scaled)\n",
        "\n",
        "# Добавляем метки кластеров в исходный DataFrame\n",
        "df_with_clusters = df.copy()\n",
        "df_with_clusters['Cluster'] = clusters\n",
        "\n",
        "# Выводим количество объектов в каждом кластере и примеры из кластеров\n",
        "print(\"Количество объектов в каждом кластере:\")\n",
        "print(df_with_clusters['Cluster'].value_counts())\n",
        "\n",
        "for i in range(3):\n",
        "    print(f\"\\nПримеры из кластера {i}:\")\n",
        "    print(df_with_clusters[df_with_clusters['Cluster'] == i].head(5))\n"
      ],
      "metadata": {
        "id": "DWssBtlXOe9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Визуализируем кластеры с помощью t-SNE при разных значениях perplexity\n",
        "perplexities = [5, 30, 50, 100]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for i, p in enumerate(perplexities):\n",
        "    tsne = TSNE(n_components=2, perplexity=p, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    labels = kmeans.labels_\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=5)\n",
        "    ax.set_title(f\"t-SNE, perplexity = {p}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "AX0Dos0_Ojm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Визуализируем кластеры с помощью t-SNE при разных значениях perplexity\n",
        "perplexities = [5, 30, 50, 100]\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "for i, p in enumerate(perplexities):\n",
        "    tsne = TSNE(n_components=2, perplexity=p, random_state=42)\n",
        "    X_tsne = tsne.fit_transform(X_scaled)\n",
        "    labels = kmeans.labels_\n",
        "    ax = axes[i // 2, i % 2]\n",
        "    scatter = ax.scatter(X_tsne[:, 0], X_tsne[:, 1], c=labels, cmap='tab10', s=5)\n",
        "    ax.set_title(f\"t-SNE, perplexity = {p}\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "QtzQpEBtOn2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Строим дендрограммы с разными методами агломерации и метриками расстояний\n",
        "methods = ['complete', 'average']\n",
        "metrics = ['euclidean', 'cosine', 'cityblock']\n",
        "\n",
        "fig, axes = plt.subplots(len(methods), len(metrics), figsize=(20, 12))\n",
        "for i, method in enumerate(methods):\n",
        "    for j, metric in enumerate(metrics):\n",
        "        ax = axes[i, j]\n",
        "        Z = linkage(X_scaled, method=method, metric=metric)\n",
        "        dendrogram(Z, ax=ax, truncate_mode='lastp', p=20)\n",
        "        ax.set_title(f\"{method} linkage - {metric} distance\")\n",
        "        ax.set_ylabel(\"Расстояние\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "LS-dKbhhOzDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Агломеративная кластеризация с 3 кластерами и визуализация по первым двум признакам\n",
        "agg_cluster = AgglomerativeClustering(n_clusters=3, metric='euclidean', linkage='ward')\n",
        "clusters_agg = agg_cluster.fit_predict(X_scaled)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=clusters_agg, cmap='Set1')\n",
        "plt.title(\"Агломеративная кластеризация (по первым двум признакам)\")\n",
        "plt.xlabel(\"Признак 1 (масштабированный)\")\n",
        "plt.ylabel(\"Признак 2 (масштабированный)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "m2Pxs2opO0Y0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Перебираем параметры eps и min_samples для DBSCAN с разными метриками\n",
        "eps_values = np.arange(0.5, 2.1, 0.1)\n",
        "min_samples_values = range(3, 11)\n",
        "metrics_list = ['euclidean', 'manhattan']\n",
        "results = []\n",
        "\n",
        "for metric in metrics_list:\n",
        "    for eps in eps_values:\n",
        "        for min_samples in min_samples_values:\n",
        "            db = DBSCAN(eps=eps, min_samples=min_samples, metric=metric)\n",
        "            labels = db.fit_predict(X_scaled)\n",
        "            mask = labels != -1\n",
        "            n_clusters = len(set(labels[mask]))\n",
        "            n_noise = np.sum(labels == -1)\n",
        "            if n_clusters > 1:\n",
        "                silhouette = silhouette_score(X_scaled[mask], labels[mask])\n",
        "                calinski = calinski_harabasz_score(X_scaled[mask], labels[mask])\n",
        "                davies = davies_bouldin_score(X_scaled[mask], labels[mask])\n",
        "            else:\n",
        "                silhouette, calinski, davies = -1, np.nan, np.nan\n",
        "            results.append({\n",
        "                'metric': metric,\n",
        "                'eps': round(eps, 2),\n",
        "                'min_samples': min_samples,\n",
        "                'n_clusters': n_clusters,\n",
        "                'n_noise': n_noise,\n",
        "                'silhouette': silhouette,\n",
        "                'calinski': calinski,\n",
        "                'davies_bouldin': davies\n",
        "            })\n",
        "\n",
        "df_dbscan = pd.DataFrame(results)\n",
        "df_dbscan.to_csv(\"dbscan_grid_search.csv\", index=False)\n",
        "print(\"\\nТоп-10 по silhouette (DBSCAN):\")\n",
        "print(df_dbscan.sort_values(by='silhouette', ascending=False).head(10))\n"
      ],
      "metadata": {
        "id": "es-rKrj8O5YU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для вычисления индекса Данна для оценки качества кластеризации\n",
        "def dunn_index(X, labels):\n",
        "    unique_clusters = np.unique(labels)\n",
        "    clusters = [np.where(labels == c)[0] for c in unique_clusters if c != -1]\n",
        "    if len(clusters) < 2:\n",
        "        return 0\n",
        "    inter_cluster_dists = []\n",
        "    for i in range(len(clusters)):\n",
        "        for j in range(i + 1, len(clusters)):\n",
        "            dist = cdist(X[clusters[i]], X[clusters[j]], metric='euclidean')\n",
        "            inter_cluster_dists.append(np.min(dist))\n",
        "    if len(inter_cluster_dists) == 0:\n",
        "        return 0\n",
        "    inter_cluster_dist = np.min(inter_cluster_dists)\n",
        "    intra_cluster_diameters = []\n",
        "    for cluster in clusters:\n",
        "        if len(cluster) > 1:\n",
        "            dists = cdist(X[cluster], X[cluster], metric='euclidean')\n",
        "            intra_cluster_diameters.append(np.max(dists))\n",
        "        else:\n",
        "            intra_cluster_diameters.append(0)\n",
        "    intra_cluster_diam = np.max(intra_cluster_diameters)\n",
        "    return inter_cluster_dist / intra_cluster_diam if intra_cluster_diam else 0\n"
      ],
      "metadata": {
        "id": "c2Z4YHtYO7DE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Сравниваю KMeans, AgglomerativeClustering, DBSCAN и OPTICS по метрикам качества кластеризации\n",
        "models = {\n",
        "    \"KMeans\": KMeans(n_clusters=3, random_state=42, n_init=10),\n",
        "    \"AgglomerativeClustering\": AgglomerativeClustering(n_clusters=3),\n",
        "    \"DBSCAN\": DBSCAN(eps=0.9, min_samples=8),\n",
        "    \"OPTICS\": OPTICS(min_samples=5)\n",
        "}\n",
        "\n",
        "silhouette_scores = {}\n",
        "davies_bouldin_scores = {}\n",
        "dunn_indices = {}\n",
        "cluster_results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    labels = model.fit_predict(X_scaled)\n",
        "    mask = labels != -1\n",
        "    silhouette_scores[name] = silhouette_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else 0\n",
        "    davies_bouldin_scores[name] = davies_bouldin_score(X_scaled[mask], labels[mask]) if len(np.unique(labels[mask])) > 1 else np.nan\n",
        "    dunn_indices[name] = dunn_index(X_scaled, labels)\n",
        "    cluster_results[name] = labels\n",
        "\n",
        "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
        "axes[0].bar(silhouette_scores.keys(), silhouette_scores.values(), color=\"skyblue\")\n",
        "axes[0].set_title(\"Silhouette Score\")\n",
        "axes[1].bar(davies_bouldin_scores.keys(), davies_bouldin_scores.values(), color=\"salmon\")\n",
        "axes[1].set_title(\"Davies-Bouldin Index\")\n",
        "axes[2].bar(dunn_indices.keys(), dunn_indices.values(), color=\"lightgreen\")\n",
        "axes[2].set_title(\"Dunn Index\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "9FJUCOP_O_p8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}